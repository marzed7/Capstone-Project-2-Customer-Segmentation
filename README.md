# Capstone-Project 2-Customer-Segmentation
 DeepLearning, MachineLearning

Welcome to my Capstone Project 2 repository. This project aims to explore, preprocess, analyze, and build predictive models for a dataset with the goal of predicting the 'Segmentation' of individuals based on various features.

## Overview

In this project, I'll be working with a dataset that contains information about individuals, including features such as 'Gender', 'Ever_Married', 'Age', 'Graduated', 'Profession', 'Work_Experience', 'Spending_Score', 'Family_Size', and 'Segmentation'. The 'Segmentation' column represents the target variable we aim to predict, which is categorized into segments A, B, C, and D.

## Goals

- Perform exploratory data analysis (EDA) to gain insights into the dataset's structure, relationships, and distributions.
- Handle missing values through appropriate strategies such as imputation and dropping.
- Preprocess the data, including one-hot encoding categorical variables and scaling numerical features.
- Build a predictive model using machine learning techniques to predict the 'Segmentation' of individuals.
- Evaluate the model's performance using relevant metrics such as accuracy and F1 score.
- Document the process, decisions, and results in a clear and concise manner.

## Repository Structure

The repository is organized as follows:

- `dataset/`: Contains the dataset used for the project.
- `models/`: DeepLearning model and MachineLearning model
- `customer_segmentation.ipynb`: Jupyter notebooks for different stages of the project, including data exploration, preprocessing, modeling, and evaluation.
- `README.md`: The main project documentation that you're currently reading.

## Model Architecture
- DeepLearning Architecture:
![image](https://github.com/marzed7/Capstone-Project-2-Customer-Segmentation/assets/141207242/172cce25-dc56-41b5-8ac5-4b5e57e4a504)

## Getting Started

To get started with this project, you can follow these steps:

1. Clone this repository to your local machine.
2. Explore the Jupyter notebooks in the directory to see the project's workflow.
3. Run the notebooks to reproduce the analysis, preprocessing, and modeling steps.
4. Feel free to modify and adapt the code to your specific needs and datasets.

## Dependencies

This project uses the following libraries and tools:

- Python
- pandas
- numpy
- matplotlib
- seaborn
- scikit-learn
- TensorFlow (for deep learning models)

Make sure to install these dependencies if you haven't already.
